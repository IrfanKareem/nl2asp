{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHb_xIBKcYuV"
      },
      "source": [
        "**IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmyDghSecUep"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "import string\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import copy\n",
        "\n",
        "!pip install OpenAI\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IylqquGScsQI"
      },
      "source": [
        "**CONECTION TO GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUGKIJ_AcsiT"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/My Drive/NL2CNL'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yXwUcy4elZI"
      },
      "source": [
        "**PROCESSING EXCEL FILE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YACh-TSeplN"
      },
      "outputs": [],
      "source": [
        "# EXTRACTING THE DATA FROM EXCEL AND CREATING A PYTHON LIST WITH OBJECTS = {ASP, CNL, NL, TCNL, TNL} Optimized Unique Temp.\n",
        "excel = pandas.read_excel(os.path.join(data_path, \"Master_File_1.xlsx\"), \"Optimized Unique Temp.\")\n",
        "\n",
        "def isNaN(string):\n",
        "    return string != string\n",
        "rows = []\n",
        "removed = []\n",
        "for index, r in excel.iterrows():\n",
        "  row = {'asp' : r[\"ASP\"] if not isNaN( r[\"ASP\"]) else ''}\n",
        "  row['cnl'] =  r[\"CNL\"] if not isNaN( r[\"CNL\"]) else ''\n",
        "  row['nl'] =  r[\"Natural Language\"] if not isNaN( r[\"Natural Language\"]) else ''\n",
        "  row['tcnl'] =  r[\"Template CNL\"] if not isNaN( r[\"Template CNL\"]) else ''\n",
        "  row['tnl'] =  r[\"Template NL\"] if not isNaN( r[\"Template NL\"]) else ''\n",
        "  rows.append(row)\n",
        "print('ROWS length >>>: ', len(rows))\n",
        "\n",
        "r_copy = rows.copy()\n",
        "for idx in range(len(rows)):\n",
        "  if(r_copy[idx]['cnl'].strip() == '' or r_copy[idx]['asp'].strip() == '' or r_copy[idx]['nl'].strip() == '' or r_copy[idx]['tcnl'].strip() == '' or r_copy[idx]['tnl'].strip() == ''):\n",
        "    removed.append(r_copy[idx])\n",
        "    rows.pop(idx)\n",
        "print('ROWS length after clean >>>: ', len(rows))\n",
        "removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-X-z7fmayut"
      },
      "outputs": [],
      "source": [
        "# ////CHECK THAT TCNL AND CNL MATCH/////////\n",
        "problematics = []\n",
        "for idx, row in enumerate(rows):\n",
        "  cnl =  re.sub('\\s+', ' ', row['cnl'])\n",
        "  tcnl = re.sub('\\s+', ' ', row['tcnl'])\n",
        "  if(cnl.strip().split()[0] != tcnl.strip().split()[0] and (not tcnl.strip().split()[0].startswith('noun') and not tcnl.strip().split()[0].startswith('num') and not tcnl.strip().split()[0].startswith('var'))):\n",
        "    problematics.append(row)\n",
        "problematics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krDUNOuVmkb_"
      },
      "outputs": [],
      "source": [
        "# EXTRACTING THE DATA FROM EXCEL CORRESPONDING TO THE ORIGINAL DATASET\n",
        "original_excel = pandas.read_excel(os.path.join(data_path, \"Master_File_1.xlsx\"), \"Templates_Duplicate_Remove\")\n",
        "\n",
        "def isNaN(string):\n",
        "    return string != string\n",
        "original_rows = []\n",
        "for index, r in original_excel.iterrows():\n",
        "  row = {'asp' : r[\"ASP\"] if not isNaN( r[\"ASP\"]) else ''}\n",
        "  row['cnl'] =  r[\"CNL\"] if not isNaN( r[\"CNL\"]) else ''\n",
        "  row['nl'] =  r[\"Natural Language\"] if not isNaN( r[\"Natural Language\"]) else ''\n",
        "  row['tcnl'] =  r[\"Template CNL\"] if not isNaN( r[\"Template CNL\"]) else ''\n",
        "  row['tnl'] =  r[\"Template NL\"] if not isNaN( r[\"Template NL\"]) else ''\n",
        "  original_rows.append(row)\n",
        "\n",
        "counts = {}\n",
        "for r in original_rows:\n",
        "  sent = re.sub('\\s+', ' ', r['nl'].strip().lower())\n",
        "  if sent not in counts:\n",
        "    counts[sent] = 1\n",
        "  else:\n",
        "    counts[sent] += 1\n",
        "\n",
        "for c in counts:\n",
        "  if counts[c] > 1:\n",
        "    print(c, ' ---------> ', counts[c])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkU0Ecy310kx"
      },
      "source": [
        "**PLOTTING THE DATASET INFORMATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osDzUCv2n-Cm"
      },
      "outputs": [],
      "source": [
        "# PLOTTING THE NL AND CNL SENTENCES LENGTH DISTRIBUTIONS\n",
        "nl_lengths = [len(row['nl'].split()) for row in rows]\n",
        "cnl_lengths = [len(row['cnl'].split()) for row in rows]\n",
        "\n",
        "# /////CALCULATING THE NUMBER OF BINS/////\n",
        "counts_nl = {}\n",
        "counts_cnl = {}\n",
        "for c in nl_lengths:\n",
        "  if str(c) not in counts_nl:\n",
        "    counts_nl[str(c)] = 1\n",
        "  else:\n",
        "    counts_nl[str(c)] = counts_nl[str(c)] + 1\n",
        "for c in cnl_lengths:\n",
        "  if str(c) not in counts_cnl:\n",
        "    counts_cnl[str(c)] = 1\n",
        "  else:\n",
        "    counts_cnl[str(c)] = counts_cnl[str(c)] + 1\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, sharey=False, tight_layout=True, figsize=(10, 5))\n",
        "a = axs[0].hist(nl_lengths, bins=len(counts_cnl))\n",
        "_ = axs[0].set_xlabel(\"Sentences length\")\n",
        "_ = axs[0].set_ylabel(\"Count\")\n",
        "_ = axs[0].set_title(\"NL senteces length distribution\")\n",
        "\n",
        "b = axs[1].hist(cnl_lengths, bins=len(counts_cnl))\n",
        "_ = axs[1].set_xlabel(\"Sentence length\")\n",
        "_ = axs[1].set_ylabel(\"Count\")\n",
        "_ = axs[1].set_title(\"CNL senteces length distribution\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvFVO7W7BFGl"
      },
      "outputs": [],
      "source": [
        "#COUNTING CNL PROPOSITIONS BY TYPE AND KEEPING THE INDEXES OF THE PROPOSITIONS IN THE LIST\n",
        "count_cnl_by_type = {\n",
        "  'Negative Strong Constraint':0,\n",
        "  'Positive Strong Constraint':0,\n",
        "  'Weak Constraint':0 ,\n",
        "  'Definition Whenever':0,\n",
        "  'Definition When':0,\n",
        "  'Definition Const/Compound':0,\n",
        "  'Quantified Choice Rules':0\n",
        "}\n",
        "negative_indexes = []\n",
        "positive_indexes = []\n",
        "weak_indexes = []\n",
        "choice_indexes = []\n",
        "whenever_indexes = []\n",
        "when_indexes = []\n",
        "other_def_indexes = []\n",
        "for idx, row in enumerate(rows):\n",
        "  cnl_lower = row['cnl'].lower().strip()\n",
        "  asp_lower = row['asp'].lower().strip()\n",
        "  if 'it is prohibited' in cnl_lower:\n",
        "    count_cnl_by_type['Negative Strong Constraint'] += 1\n",
        "    negative_indexes.append(idx)\n",
        "  elif 'it is required' in cnl_lower:\n",
        "    count_cnl_by_type['Positive Strong Constraint'] += 1\n",
        "    positive_indexes.append(idx)\n",
        "  elif 'it is preferred' in cnl_lower:\n",
        "    count_cnl_by_type['Weak Constraint'] += 1\n",
        "    weak_indexes.append(idx)\n",
        "  elif re.match(r'^\\d*\\s{0,1}\\{.*\\}', asp_lower) and '#maximize' not in asp_lower and '#minimize' not in asp_lower:\n",
        "    count_cnl_by_type['Quantified Choice Rules'] += 1\n",
        "    choice_indexes.append(idx)\n",
        "  elif cnl_lower.startswith('whenever'):\n",
        "    count_cnl_by_type['Definition Whenever'] += 1\n",
        "    whenever_indexes.append(idx)\n",
        "  elif re.match(r'.*\\swhen\\s.*$', cnl_lower):\n",
        "    count_cnl_by_type['Definition When'] += 1\n",
        "    when_indexes.append(idx)\n",
        "  else:\n",
        "     count_cnl_by_type['Definition Const/Compound'] += 1\n",
        "     other_def_indexes.append(idx)\n",
        "count_cnl_by_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2mt7-reywoY"
      },
      "outputs": [],
      "source": [
        "#COUNTING CNL PROPOSITIONS BY TYPE AND KEEPING THE INDEXES OF THE PROPOSITIONS IN THE ORIGINAL LIST (NO CLEAN)\n",
        "ori_negative_indexes = []\n",
        "ori_positive_indexes = []\n",
        "ori_weak_indexes = []\n",
        "ori_choice_indexes = []\n",
        "ori_whenever_indexes = []\n",
        "ori_when_indexes = []\n",
        "ori_other_def_indexes = []\n",
        "for idx, row in enumerate(original_rows):\n",
        "  cnl_lower = row['cnl'].lower().strip()\n",
        "  asp_lower = row['asp'].lower().strip()\n",
        "  if 'it is prohibited' in cnl_lower:\n",
        "    ori_negative_indexes.append(idx)\n",
        "  elif 'it is required' in cnl_lower:\n",
        "    ori_positive_indexes.append(idx)\n",
        "  elif 'it is preferred' in cnl_lower:\n",
        "    ori_weak_indexes.append(idx)\n",
        "  elif re.match(r'^\\d*\\s{0,1}\\{.*\\}', asp_lower) and '#maximize' not in asp_lower and '#minimize' not in asp_lower:\n",
        "    ori_choice_indexes.append(idx)\n",
        "  elif cnl_lower.startswith('whenever'):\n",
        "    ori_whenever_indexes.append(idx)\n",
        "  elif re.match(r'.*\\swhen\\s.*$', cnl_lower):\n",
        "    ori_when_indexes.append(idx)\n",
        "  else:\n",
        "     ori_other_def_indexes.append(idx)\n",
        "ori_other_def_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdfhkqMNM2T2"
      },
      "outputs": [],
      "source": [
        "# /////Plotting Number of instances by proposition type/////\n",
        "counts_nl = {}\n",
        "counts_cnl = {}\n",
        "for c in nl_lengths:\n",
        "  if str(c) not in counts_nl:\n",
        "    counts_nl[str(c)] = 1\n",
        "  else:\n",
        "    counts_nl[str(c)] = counts_nl[str(c)] + 1\n",
        "for c in cnl_lengths:\n",
        "  if str(c) not in counts_cnl:\n",
        "    counts_cnl[str(c)] = 1\n",
        "  else:\n",
        "    counts_cnl[str(c)] = counts_cnl[str(c)] + 1\n",
        "\n",
        "_ = plt.bar(list(count_cnl_by_type.keys()), list(count_cnl_by_type.values()), width=0.8)\n",
        "_ = plt.xticks(rotation=30, ha='right')\n",
        "_ = plt.title(\"Number of instances by proposition type\")\n",
        "_ = plt.ylabel(\"Count\")\n",
        "_ = plt.xlabel(\"Proposition type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOxUFVMLnBiI"
      },
      "outputs": [],
      "source": [
        "# /////SOME OTHER PLOTS/////\n",
        "from zmq.constants import XPUB_VERBOSE\n",
        "# /////INSTANCES LENGHT BY TYPE/////\n",
        "neg_lengths = np.take(cnl_lengths, negative_indexes)\n",
        "pos_lengths = np.take(cnl_lengths, positive_indexes)\n",
        "weak_lengths = np.take(cnl_lengths, weak_indexes)\n",
        "choice_lengths = np.take(cnl_lengths, choice_indexes)\n",
        "whenever_lengths = np.take(cnl_lengths, whenever_indexes)\n",
        "when_lengths = np.take(cnl_lengths, when_indexes)\n",
        "fig, axs = plt.subplots(2, 3, sharey=False, tight_layout=True, figsize=(14, 5))\n",
        "\n",
        "xs = axs[0][0].hist(neg_lengths, bins=50)\n",
        "yint = range(0, int(max(set(xs[0])))+1, 1)\n",
        "_ = axs[0][0].set_xlabel(\"Sentences length\")\n",
        "_ = axs[0][0].set_ylabel(\"Count\")\n",
        "_ = axs[0][0].set_title(\"Length distribution Negative Const Propositions\")\n",
        "_ = axs[0][0].set_yticks(yint)\n",
        "\n",
        "xs = axs[0][1].hist(pos_lengths, bins=50)\n",
        "yint = range(0, int(max(set(xs[0])))+1, 1)\n",
        "_ = axs[0][1].set_xlabel(\"Sentence length\")\n",
        "_ = axs[0][1].set_ylabel(\"Count\")\n",
        "_ = axs[0][1].set_title(\"Length distribution Positive Const Propositions\")\n",
        "_ = axs[0][1].set_yticks(yint)\n",
        "\n",
        "xs = axs[0][2].hist(weak_lengths, bins=50)\n",
        "yint = range(0, int(max(set(xs[0])))+1, 1)\n",
        "_ = axs[0][2].set_xlabel(\"Sentence length\")\n",
        "_ = axs[0][2].set_ylabel(\"Count\")\n",
        "_ = axs[0][2].set_title(\"Length distribution Weak Const Propositions\")\n",
        "_ = axs[0][2].set_yticks(yint)\n",
        "\n",
        "xs = axs[1][0].hist(choice_lengths, bins=50)\n",
        "yint = range(0, int(max(set(xs[0])))+1, 1)\n",
        "_ = axs[1][0].set_xlabel(\"Sentence length\")\n",
        "_ = axs[1][0].set_ylabel(\"Count\")\n",
        "_ = axs[1][0].set_title(\"Length distribution Choice Propositions\")\n",
        "_ = axs[1][0].set_yticks(yint)\n",
        "\n",
        "xs = axs[1][1].hist(whenever_lengths, bins=120)\n",
        "yint = range(0, int(max(set(xs[0])))+1, 1)\n",
        "_ = axs[1][1].set_xlabel(\"Sentence length\")\n",
        "_ = axs[1][1].set_ylabel(\"Count\")\n",
        "_ = axs[1][1].set_title(\"Length distribution Whenever Propositions\")\n",
        "_ = axs[1][1].set_yticks(yint)\n",
        "\n",
        "xs = axs[1][2].hist(when_lengths, bins=175)\n",
        "yint = range(0, int(max(set(xs[0])))+1, 1)\n",
        "_ = axs[1][2].set_xlabel(\"Sentence length\")\n",
        "_ = axs[1][2].set_ylabel(\"Count\")\n",
        "_ = axs[1][2].set_title(\"Length distribution When Propositions\")\n",
        "_ = axs[1][2].set_yticks(yint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFN6kaAvYa_M"
      },
      "outputs": [],
      "source": [
        "# /////ANALYSIS OF OUTLIERS/////\n",
        "'''\n",
        "pre_type is one of the following\n",
        "  'Negative Strong Constraint'\n",
        "  'Positive Strong Constraint'\n",
        "  'Weak Constraint'\n",
        "  'Definition Whenever'\n",
        "  'Definition When'\n",
        "  'Definition Const/Compound'\n",
        "  'Quantified Choice Rules'\n",
        "  '''\n",
        "def print_outliers_prepositions(pre_type, threshold=100):\n",
        "  indexes = []\n",
        "  if pre_type == 'Negative Strong Constraint':\n",
        "    indexes = negative_indexes\n",
        "  elif pre_type == 'Positive Strong Constraint':\n",
        "    indexes = positive_indexes\n",
        "  elif pre_type == 'Weak Constraint':\n",
        "    indexes = weak_indexes\n",
        "  elif pre_type == 'Definition Whenever':\n",
        "    indexes = whenever_indexes\n",
        "  elif pre_type == 'Definition When':\n",
        "    indexes = when_indexes\n",
        "  elif pre_type == 'Quantified Choice Rules':\n",
        "    indexes = choice_indexes\n",
        "  else:\n",
        "    indexes = other_def_indexes\n",
        "\n",
        "  lengths = np.take(cnl_lengths, indexes)\n",
        "  for idx, l in enumerate(lengths):\n",
        "    if l > threshold:\n",
        "      print('index: ', indexes[idx], ' length: ', l, ' sentence: ', rows[indexes[idx]]['cnl'])\n",
        "\n",
        "print_outliers_prepositions('Definition Whenever', 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s2QTCddgq6e"
      },
      "source": [
        "**DATASET GENERATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2msciiu0gxi1"
      },
      "source": [
        "**Support Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKeEsQjagqC7"
      },
      "outputs": [],
      "source": [
        "# ////Integer/Integer-Interval generators////\n",
        "# ///This functions allow to generate the integers and range of integet to be used in the placeholder replacement///\n",
        "import random\n",
        "def generate_random_integer(start=1, end=50):\n",
        "  return random.randrange(start, end)\n",
        "\n",
        "def generate_random_interval(min=1, max=100):\n",
        "  start = generate_random_integer(min, max)\n",
        "  end = generate_random_integer(1, max-start+1)\n",
        "  while start+end > max:\n",
        "    end = generate_random_integer(1, max-start+1)\n",
        "  return [start, end+start]\n",
        "\n",
        "generate_random_interval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTy5e23egmCB"
      },
      "source": [
        "**Balanced Dataset Generation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1byMSLE4azLO"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "{\n",
        "  verbs: list,\n",
        "  nouns: list,\n",
        "  pid: list\n",
        "}\n",
        "'''\n",
        "\n",
        "def generate_replacements(bow, cnl_template, nl_template):\n",
        "    # Combine CNL and NL templates for extracting placeholders\n",
        "    template = cnl_template + nl_template\n",
        "\n",
        "    replacements = {}\n",
        "\n",
        "    # Replacing verb_X placeholders with appropriate values\n",
        "    verb_placeholders = re.findall(r'\\bverb_\\d+', template)\n",
        "    verb_placeholders = set(verb_placeholders)\n",
        "    useds = []\n",
        "    print('---Replacing verbs---')\n",
        "    for placeholder in verb_placeholders:\n",
        "        choice = random.choice(bow['verbs'])\n",
        "        while choice in useds:\n",
        "          choice = random.choice(bow['verbs'])\n",
        "        replacements[placeholder] = choice\n",
        "        useds.append(choice)\n",
        "\n",
        "    # Replacing noun_X placeholders with appropriate values\n",
        "    noun_placeholders = re.findall(r'\\bnoun_\\d+', template)\n",
        "    noun_placeholders = set(noun_placeholders)\n",
        "    useds = []\n",
        "    print('---Replacing nouns---')\n",
        "    for placeholder in noun_placeholders:\n",
        "        choice = random.choice(bow['nouns'])\n",
        "        while choice in useds:\n",
        "          choice =  random.choice(bow['nouns'])\n",
        "        replacements[placeholder] = choice\n",
        "        useds.append(choice)\n",
        "\n",
        "    # Replacing var_X placeholders with appropriate values\n",
        "    var_placeholders = re.findall(r'\\bvar_\\d+', template)\n",
        "    var_placeholders = set(var_placeholders)\n",
        "    useds = []\n",
        "    print('---Replacing variables---')\n",
        "    for placeholder in var_placeholders:\n",
        "        choice = random.choice(string.ascii_uppercase)\n",
        "        while choice in useds:\n",
        "          choice = random.choice(string.ascii_uppercase)\n",
        "        replacements[placeholder] = choice\n",
        "        useds.append(choice)\n",
        "\n",
        "    # Replacing col_X placeholders with appropriate values\n",
        "    col_placeholders = re.findall(r'\\bcol_\\d+', template)\n",
        "    useds = []\n",
        "    print('---Replacing colors---')\n",
        "    for placeholder in col_placeholders:\n",
        "        choice = random.choice(['red', 'green', 'blue', 'yellow', 'orange', 'purple', 'pink', 'brown', 'black'])\n",
        "        while choice in useds:\n",
        "          choice = random.choice(['red', 'green', 'blue', 'yellow', 'orange', 'purple', 'pink', 'brown', 'black'])\n",
        "        replacements[placeholder] = choice\n",
        "        useds.append(choice)\n",
        "\n",
        "    # Replacing num_X placeholders with appropriate values\n",
        "    num_placeholders = re.findall(r'\\bnum_\\d+', template)\n",
        "    num_placeholders = set(num_placeholders)\n",
        "    print('---Replacing numbers---')\n",
        "    for i, placeholder in enumerate(sorted(num_placeholders)):\n",
        "        replacements[placeholder] = generate_random_integer()\n",
        "\n",
        "    # Replace num_range placeholders with appropriate values\n",
        "    num_range_placeholders = re.findall(r'\\bnum_range\\b', template)\n",
        "    num_range_placeholders = set(num_range_placeholders)\n",
        "    print('---Replacing number ranges---')\n",
        "    for placeholder in num_range_placeholders:\n",
        "        interval = generate_random_interval()\n",
        "        replacements[placeholder] = f'{interval[0]} to {interval[1]}'\n",
        "\n",
        "    # Replace num_choice placeholders with appropriate values\n",
        "    num_choice_placeholders = re.findall(r'\\bnum_choice\\b', template)\n",
        "    print('---Replacing number choice constructs---')\n",
        "    for placeholder in num_choice_placeholders:\n",
        "        num_list = [generate_random_integer(1, 30) for _ in range(random.randint(2, 10))]\n",
        "        num_list = sorted(list(set(num_list)))\n",
        "        replacements[placeholder] = ', '.join(map(str, num_list))\n",
        "\n",
        "    # Replace PID_X placeholders with appropriate values\n",
        "    pid_placeholders = re.findall(r'\\bPID_\\d+', template)\n",
        "    pid_placeholders = set(pid_placeholders)\n",
        "    useds = []\n",
        "    print('---Replacing identifiers---')\n",
        "    for placeholder in pid_placeholders:\n",
        "        choice = random.choice(bow['pid'])\n",
        "        while choice in useds:\n",
        "          choice = random.choice(bow['pid'])\n",
        "        replacements[placeholder] = choice\n",
        "        useds.append(choice)\n",
        "    return replacements\n",
        "\n",
        "\n",
        "def fill_template(template, replacements):\n",
        "    filled_template = template\n",
        "    for placeholder, value in replacements.items():\n",
        "        # Convert the value to string if it's an integer\n",
        "        if isinstance(value, int):\n",
        "            value = str(value)\n",
        "        filled_template = filled_template.replace(placeholder, value)\n",
        "    return filled_template\n",
        "\n",
        "\n",
        "# Loading predefined words from files\n",
        "verbs_df = pandas.read_excel('/content/drive/MyDrive/NL2CNL/verbs.xlsx')  # Excel file containing verbs\n",
        "nouns_df = pandas.read_excel('/content/drive/MyDrive/NL2CNL/nouns.xlsx')  # Excel file containing nouns\n",
        "pid_df = pandas.read_excel('/content/drive/MyDrive/NL2CNL/pid.xlsx')  # Excel file containing pid\n",
        "\n",
        "# Extract verbs and nouns from dataframes\n",
        "verbs = verbs_df['Verb'].tolist()\n",
        "nouns = nouns_df['Noun'].tolist()\n",
        "pid = pid_df['PID'].tolist()\n",
        "\n",
        "# Bag of Words\n",
        "bow = {'nouns': nouns, 'verbs': verbs, 'pid': pid}\n",
        "\n",
        "# Set the target number of instances for each category\n",
        "target_count = 180\n",
        "\n",
        "# # Initialize a dictionary to store the balanced dataset\n",
        "balanced_rows = defaultdict(list)\n",
        "\n",
        "# ////Adding the former NL and CNL to the temporal list\n",
        "generateds_tmp = [re.sub(' +', ' ', row['nl']).strip().lower() for row in original_rows] + [re.sub(' +', ' ', row['cnl']).strip().lower() for row in original_rows]\n",
        "\n",
        "# GENERATING AND BALANCING THE DATASET\n",
        "\n",
        "# number of instances by preposition type in the handmade dataset\n",
        "former_count_by_preps = {'Negative Strong Constraint': 22,\n",
        "                         'Positive Strong Constraint': 39,\n",
        "                         'Weak Constraint': 11,\n",
        "                         'Definition Whenever': 119,\n",
        "                         'Definition When': 145,\n",
        "                         'Definition Const/Compound': 154,\n",
        "                         'Quantified Choice Rules': 4}\n",
        "\n",
        "\n",
        "cat_idx_pairs = zip(['Negative Strong Constraint', 'Positive Strong Constraint', 'Weak Constraint',\n",
        "                                'Definition Whenever', 'Definition When', 'Quantified Choice Rules'],\n",
        "                              [negative_indexes, positive_indexes, weak_indexes, whenever_indexes, when_indexes, choice_indexes])\n",
        "max_count = 160\n",
        "for category, indexes in cat_idx_pairs:\n",
        "    count = 0\n",
        "    target_count = max_count - former_count_by_preps[category]\n",
        "    # /////Make at this point the checkings to avoid repetition\n",
        "    while count < target_count:\n",
        "    # while count < target_count:\n",
        "        index = random.choice(indexes)\n",
        "\n",
        "        row = rows[index]\n",
        "        tcnl = row['tcnl']\n",
        "        tnl = row['tnl']\n",
        "\n",
        "        replacements = generate_replacements(bow, tcnl, tnl)\n",
        "        filled_cnl = fill_template(tcnl, replacements)\n",
        "        filled_nl = fill_template(tnl, replacements)\n",
        "\n",
        "        # If the generated sentences already exists, then generate skip it (Avoiding duplicates)\n",
        "        if filled_cnl.strip().lower() not in generateds_tmp and filled_nl.strip().lower() not in generateds_tmp:\n",
        "          # Append the filled templates to the balanced dataset\n",
        "          balanced_rows[category].append({'CNL': filled_cnl, 'NL': filled_nl, 'TCNL': tcnl, 'TNL': tnl, 'ASP': row['asp'], 'Generated': True})\n",
        "          generateds_tmp.append(re.sub(' +', ' ', filled_cnl).strip().lower())\n",
        "          generateds_tmp.append(re.sub(' +', ' ', filled_nl).strip().lower())\n",
        "          count += 1\n",
        "\n",
        "# ///Generation of Specifice Categories, to create the balance dataset.\n",
        "former_count_by_preps = {'Definition Const/Compound': 154, 'Quantified Choice Rules': 4}\n",
        "cat_idx_pairs = zip(['Definition Const/Compound'], [other_def_indexes])\n",
        "for category, indexes in cat_idx_pairs:\n",
        "    count = 0\n",
        "    seq_idx = 0\n",
        "    target_count = max_count - former_count_by_preps[category]\n",
        "    # /////Make at this point the checkings to avoid repetition\n",
        "    while count < target_count:\n",
        "    # while count < target_count:\n",
        "        if(seq_idx ==len(other_def_indexes)):\n",
        "          seq_idx = 0\n",
        "        index = indexes[seq_idx]\n",
        "        row = rows[index]\n",
        "\n",
        "        tcnl = row['tcnl']\n",
        "        tnl = row['tnl']\n",
        "        replacements = generate_replacements(bow, tcnl, tnl)\n",
        "\n",
        "        filled_cnl = fill_template(tcnl, replacements)\n",
        "        filled_nl = fill_template(tnl, replacements)\n",
        "\n",
        "        # If the generated sentences already exists, then generate skip it (Avoiding duplicates)\n",
        "        if filled_cnl.strip().lower() not in generateds_tmp and filled_nl.strip().lower() not in generateds_tmp:\n",
        "          # Append the filled templates to the balanced dataset\n",
        "          balanced_rows[category].append({'CNL': filled_cnl, 'NL': filled_nl, 'TCNL': tcnl, 'TNL': tnl, 'ASP': row['asp'], 'Generated': True})\n",
        "          generateds_tmp.append(re.sub(' +', ' ', filled_cnl).strip().lower())\n",
        "          generateds_tmp.append(re.sub(' +', ' ', filled_nl).strip().lower())\n",
        "          count += 1\n",
        "        seq_idx+=1\n",
        "\n",
        "# //////creating some extra \"goes from\"/////\n",
        "count = 0\n",
        "while count < 15:\n",
        "    tcnl = 'A noun_1 goes from num_range.'\n",
        "    tnl = 'Assume that there are num_range noun_1 in a graph.'\n",
        "    asp = 'vtx(1..4).'\n",
        "    replacements = generate_replacements(bow, tcnl, tnl)\n",
        "\n",
        "    filled_cnl = fill_template(tcnl, replacements)\n",
        "    filled_nl = fill_template(tnl, replacements)\n",
        "\n",
        "    # If the generated sentences already exists, then generate them again (Avoiding duplicates)\n",
        "    if filled_cnl.strip().lower() not in generateds_tmp and filled_nl.strip().lower() not in generateds_tmp:\n",
        "      # Append the filled templates to the balanced dataset\n",
        "      balanced_rows[category].append({'CNL': filled_cnl, 'NL': filled_nl, 'TCNL': tcnl, 'TNL': tnl, 'ASP': asp, 'Generated': True})\n",
        "      generateds_tmp.append(re.sub(' +', ' ', filled_cnl).strip().lower())\n",
        "      generateds_tmp.append(re.sub(' +', ' ', filled_nl).strip().lower())\n",
        "      count += 1\n",
        "# /////////////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "# Convert the balanced dataset to a pandas DataFrame and save it as a CSV file\n",
        "balanced_data = []\n",
        "for category, rows in balanced_rows.items():\n",
        "    for row in rows:\n",
        "        balanced_data.append({'Category': category, 'CNL': row['CNL'], 'NL': row['NL'], 'TCNL': row['TCNL'], 'TNL':  row['TNL'], 'ASP': row['ASP'], 'Generated': True})\n",
        "\n",
        "\n",
        "# # Combine original dataset the balanced dataset to a pandas DataFrame and save it as a CSV file\n",
        "ori_cat_idx_pairs = zip(['Negative Strong Constraint', 'Positive Strong Constraint', 'Weak Constraint',\n",
        "                                'Definition Whenever', 'Definition When', 'Definition Const/Compound',\n",
        "                                'Quantified Choice Rules'],\n",
        "                              [ori_negative_indexes, ori_positive_indexes, ori_weak_indexes, ori_whenever_indexes, ori_when_indexes, ori_other_def_indexes, ori_choice_indexes])\n",
        "\n",
        "for category, indexes in ori_cat_idx_pairs:\n",
        "  for idx in indexes:\n",
        "    balanced_data.append({'Category': category, 'CNL': original_rows[idx]['cnl'], 'NL': original_rows[idx]['nl'], 'TCNL': original_rows[idx]['tcnl'],\n",
        "                          'TNL':  original_rows[idx]['tnl'], 'ASP': original_rows[idx]['asp'], 'Generated': False})\n",
        "\n",
        "\n",
        "# //////FIXING HAVE/HAS - THERE IS - THERE SHOULD BE//////\n",
        "def fix_syntax_has_have(sentence, reg_exp='(have|has)\\s+(an|a)\\s+[^\\s]+', casual=False):\n",
        "  matches = re.finditer(reg_exp, sentence)\n",
        "  nl = sentence\n",
        "  for mtch in matches:\n",
        "    do_replace = True if casual==False else bool(random.getrandbits(1))\n",
        "    spl = mtch.group(0).split()\n",
        "    rpl = ''\n",
        "    if do_replace:\n",
        "      if spl[2][0] in [\"a\", \"e\", \"i\", \"o\", \"u\", \"A\", \"E\", \"I\", \"O\", \"U\"]:\n",
        "        rpl = 'has an ' + spl[2]\n",
        "      else:\n",
        "        rpl = 'has a ' + spl[2]\n",
        "      nl = nl.replace(mtch.group(0), rpl)\n",
        "      nl = nl.replace('then we must has', 'then we must have')\n",
        "      nl = nl.replace('that there will not has', 'that there will not have')\n",
        "      nl = nl.replace('to has', 'to have')\n",
        "  return nl\n",
        "\n",
        "def fix_syntax_there_is(sentence, reg_exp='(there is)\\s+(an|a)\\s+[^\\s]+', casual=False):\n",
        "  matches = re.finditer(reg_exp, sentence)\n",
        "  nl = sentence\n",
        "  for mtch in matches:\n",
        "    do_replace = True if casual==False else bool(random.getrandbits(1))\n",
        "    spl = mtch.group(0).split()\n",
        "    rpl = ''\n",
        "    if do_replace:\n",
        "      if spl[3][0] in [\"a\", \"e\", \"i\", \"o\", \"u\", \"A\", \"E\", \"I\", \"O\", \"U\"]:\n",
        "        rpl = 'there is an ' + spl[3]\n",
        "      else:\n",
        "        rpl = 'there is a ' + spl[3]\n",
        "      nl = nl.replace(mtch.group(0), rpl)\n",
        "  return nl\n",
        "\n",
        "def fix_syntax_can_be(sentence, reg_exp='(can be)\\s+(an|a)\\s+[^\\s]+', casual=False):\n",
        "  matches = re.finditer(reg_exp, sentence)\n",
        "  nl = sentence\n",
        "  for mtch in matches:\n",
        "    do_replace = True if casual==False else bool(random.getrandbits(1))\n",
        "    spl = mtch.group(0).split()\n",
        "    rpl = ''\n",
        "    if do_replace:\n",
        "      if spl[3][0] in [\"a\", \"e\", \"i\", \"o\", \"u\", \"A\", \"E\", \"I\", \"O\", \"U\"]:\n",
        "        rpl = 'can be an ' + spl[3]\n",
        "      else:\n",
        "        rpl = 'can be a ' + spl[3]\n",
        "      nl = nl.replace(mtch.group(0), rpl)\n",
        "  return nl\n",
        "\n",
        "def replace_priority(dataset):\n",
        "   medium_count = 53\n",
        "   high_count = 40\n",
        "   do_high = True\n",
        "   for idx, data in enumerate(dataset):\n",
        "      nl =  data['NL']\n",
        "      cnl =  data['CNL']\n",
        "      if 'low priority' in cnl.lower():\n",
        "        if do_high and high_count > 0:\n",
        "          data['NL'] = nl.replace('low priority', 'high priority')\n",
        "          data['CNL'] = cnl.replace('low priority', 'high priority')\n",
        "          high_count -= 1\n",
        "          do_high = False\n",
        "        elif do_high == False and medium_count > 0:\n",
        "          data['NL'] = nl.replace('low priority', 'medium priority')\n",
        "          data['CNL'] = cnl.replace('low priority', 'medium priority')\n",
        "          medium_count -= 1\n",
        "          if high_count > 0:\n",
        "            do_high = True\n",
        "        dataset[idx] = data\n",
        "   return dataset\n",
        "\n",
        "# to replace less than with at most and not after\n",
        "def replace_priority2(dataset):\n",
        "   at_most = 32\n",
        "   not_after = 32\n",
        "   do_high = True\n",
        "   for idx, data in enumerate(balanced_data):\n",
        "      nl =  data['NL']\n",
        "      cnl =  data['CNL']\n",
        "      if 'less than' in cnl.lower():\n",
        "        if do_high and at_most > 0:\n",
        "          data['NL'] = nl.replace('less than', 'at most')\n",
        "          data['CNL'] = cnl.replace('less than', 'at most')\n",
        "          at_most -= 1\n",
        "          do_high = False\n",
        "        elif do_high == False and not_after > 0:\n",
        "          data['NL'] = nl.replace('less than', 'not after')\n",
        "          data['CNL'] = cnl.replace('less than', 'not after')\n",
        "          not_after -= 1\n",
        "          if not_after > 0:\n",
        "            do_high = True\n",
        "        dataset[idx] = data\n",
        "   return dataset\n",
        "\n",
        "\n",
        "def fix_spelling(inst):\n",
        "  cnl =  re.sub('\\s+', ' ', inst['CNL'])\n",
        "  nl = re.sub('\\s+', ' ', inst['NL'])\n",
        "  # /////HAS/HAVE////\n",
        "  reg_exp = '(have|has)\\s+(an|a)\\s+[^\\s]+'\n",
        "  cnl = fix_syntax_has_have(cnl, casual=True)\n",
        "  nl = fix_syntax_has_have(nl)\n",
        "  cnl = fix_syntax_there_is(cnl, casual=True)\n",
        "  nl = fix_syntax_there_is(nl)\n",
        "  cnl = fix_syntax_can_be(cnl, casual=True)\n",
        "  nl = fix_syntax_can_be(nl)\n",
        "  inst['CNL'] = cnl\n",
        "  inst['NL'] = nl\n",
        "  return inst\n",
        "\n",
        "for idx, data in enumerate(balanced_data):\n",
        "  balanced_data[idx] = fix_spelling(data)\n",
        "  # print(data)\n",
        "balanced_data = replace_priority(balanced_data)\n",
        "balanced_data = replace_priority2(balanced_data)\n",
        "\n",
        "balanced_df = pandas.DataFrame(balanced_data)\n",
        "balanced_df.to_csv('Balanced_Dataset_Check.csv', index=False)\n",
        "balanced_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7iez3P1_aAD"
      },
      "outputs": [],
      "source": [
        "len(balanced_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnmMGfGS9fXo"
      },
      "source": [
        "**Rephrasing The NL Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H51cV9TL5kK6"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Define your OpenAI API key\n",
        "openai.api_key = 'OPENAI API KEY HERE'\n",
        "\n",
        "def rephrase_sentence(sentence, full_nls):\n",
        "    rephrases = []\n",
        "    rephrases_lower = []\n",
        "    while len(rephrases) < 5:\n",
        "        response = openai.Completion.create(\n",
        "            engine='text-davinci-003',\n",
        "            prompt=f\"Rephrase the following sentence: {sentence}\",\n",
        "            temperature=0.6,\n",
        "            max_tokens=1000,\n",
        "            n=1,\n",
        "            stop=None,\n",
        "        )\n",
        "        rephrased_sentence = re.sub('\\s+', ' ', response.choices[0].text.strip())\n",
        "        if (rephrased_sentence.lower() not in rephrases_lower) and (rephrased_sentence.lower() not in full_nls):\n",
        "            rephrases.append(rephrased_sentence)\n",
        "            rephrases_lower.append(rephrased_sentence.lower())\n",
        "        time.sleep(0.8)\n",
        "    return rephrases\n",
        "\n",
        "# Rephrase sentences and add a new column RNL\n",
        "full_bal_nls = [re.sub('\\s+', ' ', row['NL'].lower().strip()) for row in balanced_data]\n",
        "for row in balanced_data:\n",
        "    rephrases = rephrase_sentence(row['NL'], full_bal_nls)\n",
        "    full_bal_nls = full_bal_nls + rephrases\n",
        "    for reph in rephrases:\n",
        "      tmp_row = copy.deepcopy(row)\n",
        "      tmp_row[\"NL\"] = reph\n",
        "      balanced_data.append(tmp_row)\n",
        "\n",
        "Save the updated dataset\n",
        "df.to_csv('rephrased_dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtUAgiFwRUjQ"
      },
      "outputs": [],
      "source": [
        "balanced_data = pandas.read_csv('/content/Balanced_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(balanced_data)"
      ],
      "metadata": {
        "id": "zcHHg5dmom2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZkfokf19HMW"
      },
      "source": [
        "**Creating Combine JSON (Original + Generated)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHRIPp0r0qfE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Converting the dictionary to a JSON string\n",
        "dataset = {'version': 1.0, 'name': 'NL2CNL Dataset', 'pairs': balanced_data }\n",
        "\n",
        "# Saving final dataset in JSON version\n",
        "with open('full_dataset.json', 'w') as file:\n",
        "    file.write(json.dumps(dataset))\n",
        "print(json.dumps(dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FINAL DATASET ANALYSIS**"
      ],
      "metadata": {
        "id": "i7AFuIukK7Kq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMHQG-gzcfLg"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load the balanced dataset\n",
        "balanced_df = pd.read_csv('Balanced_Dataset.csv')\n",
        "\n",
        "# Count the number of instances in each category\n",
        "category_counts = balanced_df['Category'].value_counts().reset_index()\n",
        "\n",
        "# Rename columns for better readability\n",
        "category_counts.columns = ['Category', 'Count']\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x='Category', y='Count', data=category_counts, width=0.6)\n",
        "plt.xticks(rotation=30)\n",
        "plt.xlabel('CNL Category')\n",
        "plt.ylabel('Number of Instances')\n",
        "plt.title('Number of Instances per CNL Category')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot as an image file\n",
        "plt.savefig('CNL_Category_Bar_Plot.png', dpi=300)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9EyOtzzdd23"
      },
      "outputs": [],
      "source": [
        "# Calculate the length of NL and CNL sentences\n",
        "balanced_df['CNL_length'] = balanced_df['CNL'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
        "balanced_df['NL_length'] = balanced_df['NL'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
        "\n",
        "\n",
        "# Plot the distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
        "\n",
        "# CNL length distribution\n",
        "sns.histplot(data=balanced_df, x='CNL_length', kde=True, ax=axes[0], bins=20)\n",
        "axes[0].set_title('CNL Sentence Length Distribution')\n",
        "axes[0].set_xlabel('CNL Length')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# NL length distribution\n",
        "sns.histplot(data=balanced_df, x='NL_length', kde=True, ax=axes[1], bins=20)\n",
        "axes[1].set_title('NL Sentence Length Distribution')\n",
        "axes[1].set_xlabel('NL Length')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot as an image file\n",
        "plt.savefig('Sentence_Length_Distributions.png', dpi=300)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}